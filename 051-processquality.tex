Some partial analyses proved the reactivity of the (English) project,
especially to cope with damages, because most of the articles are
regularly edited: \citet{Burioletal06} indicated, for instance, that
over 80\% of the articles were updated in a three-months period. Thanks
to this constant vigilance, \citet{Viegasetal07} showed that ''mass
deletions were reverted in a median time of 2.8 minutes, and obscene
mass deletions were reverted in a median time of 1.7 minutes'' (p.
4). \citet{Priedhorskyetal07} confirmed this speed, as they found
that 42\% of damage incidents are repaired almost immediately (i.e.,
within one estimated view, i.e. one person looking at it). However,
they also found that ''11\% of incidents persist beyond 100 views'',
but only ''0.75 \textendash{} beyond 1000 views'' (see figure 7
page 7 of their article). Going a step further, \citet{Halavais04}
introduced errors in article, which were all removed within 48h. \citet{Magnus08}
improved the process, making the group of errors less easy to detect,
and noted also good reactivity. If these analyses prove Wikipedia
organization's efficiency to preserve the existing stock of knowledge,
they do not give information about the process to produce new stock. 

\citet{Suhetal09} showed that ''both the rate of page growth and
editor growth has declined'' in the English Wikipedia. As we viewed
in this document, this is a strong indicator of growing coordination
costs, with an increase of the structure of governance and of the
discussions in the talk pages, at least for the main projects, several
points indicating that some projects may be in the decreasing phase
of the S-shape production curve. \citet{CrowstonJullienOrtega13},
using digital environment analyses tried to model the production curve,
or more exactly to compare the efficiency of the different language
projects, and confirmed that some of the main projects are less efficient,
but without being able to make this a rule, especially when taking
into account the quality of the articles (the FA). If these studies
represent a first step toward evaluation the efficiency of this online
community, this is one of the less explored field in Wikipedia studies.

One of the reasons for the few number of studies may be the difficulty
to evaluate this quality, whose perception is socially constructed
and constantly evolving \citep{Stviliaetal08,Stviliaetal09}, as the
process behind, the information quality process as defined by these
authors \citep{StviliaGasser08}. \citet{Stviliaetal08} proposed
an in-depth analysis of the rules, the structures, and the interaction
behind the creation of Wikipedia articles, and especially the Feature
Articles. They concluded that Wikipedia has, today ''not only a set
of formal quality assessment criteria, but also a whole infrastructure
of quality evaluation and promotion mechanisms and guides'' (p. 997),
supported by a specific interplay between the technology and community
mechanisms, which created ''a new system of IQ assurance that is
robust and that promotes continuous IQ improvement'' (p. 999). For
instance, the double level of management, but also the small group
control of articles and projects help to understand how Wikipedia
copes with producing articles of quality without identified authors,
when \citep{Miller05}, summarized by \citet{deLaat10}, pointed the
fact that the trust comes with a decrease of the anonymity.  If the
evaluation of the efficiency of this system in terms of productivity
is still a work in progress, several studies exist on the evaluation
of the stock of knowledge produced, and on the quality of the experience
of its users. 